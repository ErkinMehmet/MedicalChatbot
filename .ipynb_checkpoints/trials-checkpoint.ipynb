{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bcb1c-8fa8-42db-aca6-ea17a522b48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343954e-006f-4446-89e0-2d2a955ad719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone,os\n",
    "from langchain.document_loaders import PyMuPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1647926-8180-4a7b-957d-9b87e5675bf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mchatbot (3.10.0) (Python 3.10.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/hli/OneDrive - FQM/Bureau/Learning/MLOps/MedicalChatbot/mchatbot/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
    "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV')\n",
    "\n",
    "# load data from pdf\n",
    "def extract_data_pdf(path):\n",
    "    loader=DirectoryLoader(path,glob=\"*.pdf\",loader_cls=PyMuPDFLoader)\n",
    "    documents=loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data=extract_data_pdf(\"data/\")\n",
    "\n",
    "# create text chunks\n",
    "def text_split(pdf_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(pdf_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks=text_split(extracted_data)\n",
    "print(len(text_chunks))\n",
    "\n",
    "# download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MinLM-L6-v2\")\n",
    "    return embeddings\n",
    "embeddings=download_hugging_face_embeddings()\n",
    "embeddings\n",
    "\n",
    "query_result=embeddings.embed_query(\"Hello world\")\n",
    "print(len(query_result)) # 1-D matrix that represents the context (different from the GPT-2 architecture)\n",
    "\n",
    "# init Pinecone\n",
    "pinecone.init(api_key=PINECONE_API_KEY,environment=PINECONE_API_ENV)\n",
    "index_name=\"medical-chatbot\" # with aws\n",
    "# creating embeddings for each of the text chunks and storing\n",
    "docsearch=Pinecone.from_texts([t.page_content for t in text_chunks],embeddings,index_name=index_name)\n",
    "#If we already have an index we can load it like this\n",
    "docsearch=Pinecone.from_existing_index(index_name,embeddings)\n",
    "query=\"What are Allergies\"\n",
    "docs=docsearch.similarity_search(query,k=3) # we recover 3 documents\n",
    "print(\"Result\",docs)\n",
    "\n",
    "prompt_template=\"\"\"\n",
    "    Use the following pieces of information to answer the user's question.\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    \n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \n",
    "    Only return the helpful answer below and nothing else.\n",
    "    Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT=PromptTemplate(template=prompt_template,input_variables=[\"context\",\"question\"]) # tells us which placeholders are for documents and query\n",
    "chain_type_kwargs={\"prompt\":PROMPT}\n",
    "llm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",model_type=\"llama\",config={'max_new_tokens':512,'temperature':0.8})\n",
    "qa=RetrievalQA.from_chain_type(llm=llm,chain_type=\"stuff\", # multiple documents will be pasted together\n",
    "                               retriever=docsearch.as_retriever(search_kwargs={'k':2}), # 2 most relevant documents will be returned\n",
    "                               return_source_documents=True,chain_type_kwargs=chain_type_kwargs) # and they will go in the context placeholder\n",
    "while True:\n",
    "    user_input=input(f\"Input prompt:\")\n",
    "    result=qa({\"query\":user_input}) # query replaces the place holder of question\n",
    "    print(\"response:\",result[\"result\"])\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mchatbot (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
